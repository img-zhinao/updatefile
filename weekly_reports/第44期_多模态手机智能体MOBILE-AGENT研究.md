# **《智脑时代周刊》第44期**

# **多模态手机智能体MOBILE-AGENT：技术演进、架构解析与前沿展望**

##                                                                                                                             编制：卢向彤2025.5.13

## **1\. 引言：多模态手机智能体的兴起**

### **1.1 移动设备上智能助手的崛起**

智能手机已成为现代生活不可或缺的核心组成部分，深刻改变了人们获取信息、处理事务和进行娱乐的方式 1。随着用户对移动设备功能需求的日益增长，传统基于简单触摸或预设语音指令的交互模式已难以满足复杂、多步骤任务的处理需求。早期的数字助手虽然提供了一定的便利，但在理解用户深层意图、处理复杂操作流程方面存在显著局限性，往往只能完成设定闹钟或发送短信等相对简单的任务 3。

近年来，大型多模态模型（Large Multimodal Models, LMMs）的迅猛发展为构建更强大、更智能的移动设备智能体带来了革命性的契机 1。这些先进的模型具备感知和行动能力，能够在移动环境中代表用户执行任务 1。通过融合文本、图像乃至语音等多种信息模态，LMMs为智能体赋予了前所未有的环境理解和交互能力，推动了移动智能助手向更高阶的自主性与智能化演进。

### **1.2 多模态交互的定义及其对移动智能体的重要性**

“多模态”交互指的是智能系统能够处理并整合来自不同信息通道（或称“模态”）的数据，例如文本指令、屏幕截图（图像信息），甚至在某些情况下还包括语音输入 4。对于移动智能体而言，多模态交互能力至关重要。移动应用程序的用户界面（Graphical User Interfaces, GUIs）本质上是视觉化的，包含丰富的文本、图标、布局和动态元素。因此，智能体必须具备“看见”并“理解”屏幕内容的能力，才能有效地在各种应用程序中执行操作 4。

多模态移动智能体的核心目标是直接感知视觉信号，并基于此与用户及图形用户界面进行交互 6，力求模拟人类在操作手机时的自然交互方式 7。这种能力使得智能体不再局限于预定义的命令集或结构化的数据输入，而是能够根据当前屏幕的实际视觉呈现来做出决策和执行动作。

### **1.3 从指令驱动到感知驱动的交互范式转变**

观察早期移动助手与基于LMMs的新一代移动智能体（如MOBILE-AGENT系列）的差异，可以发现一个显著的交互范式转变：从主要依赖预设指令的模式，过渡到以环境感知和理解为核心的驱动模式。传统的移动助手，例如用于设置闹钟或发送文本消息的助手 3，其操作基于用户明确的、结构化的指令，对屏幕的视觉上下文理解非常有限。

大型多模态模型，特别是那些集成了视觉理解能力的模型 4，赋予了智能体处理和理解屏幕截图等视觉信息的能力。MOBILE-AGENT等系统正是利用了这一点，通过内置的视觉感知工具来准确识别和定位屏幕上的视觉元素和文本元素 4。这意味着智能体的行动决策更多地依赖于当前屏幕上实际显示的内容，而非仅仅依赖于结构化命令或底层系统元数据。因此，交互模型从“执行X操作”（X是一个已知的、预定义的指令）演变为“基于你所看到的屏幕内容来达成Y目标”（Y是一个用户意图，智能体通过感知UI来规划具体步骤）。这种转变使得智能体能够更好地适应多样化和动态变化的应用程序环境，因为它们不再受限于对每个应用程序结构的预编程知识，从而有潜力处理更广泛、更复杂的任务。

## **2\. MOBILE-AGENT系列：起源与演进**

### **2.1 MOBILE-AGENT：基本原理与视觉中心方法**

MOBILE-AGENT是一个自主的多模态移动设备智能体，其核心设计理念是利用视觉感知工具来精确识别和定位应用程序前端界面中的视觉与文本元素 4。基于这种感知到的视觉上下文，MOBILE-AGENT能够自主地规划和分解复杂的操作任务，并通过一系列步骤在移动应用程序中导航和执行操作 4。

#### **2.1.1 摆脱系统依赖的方法**

MOBILE-AGENT的一个关键创新在于其操作方式完全基于设备屏幕截图，从而避免了对底层系统文件（如XML布局文件）或移动操作系统元数据的依赖 5。这种视觉中心的方法论显著增强了智能体在不同移动操作系统和设备环境中的适应性，消除了进行特定于系统的定制化需求 7。这使得MOBILE-AGENT有潜力应用于更广泛的设备和应用生态系统。

### **2.2 Mobile-Agent-E：迈向复杂性与自给自足**

为了克服早期移动智能体在处理现实世界复杂需求时所面临的挑战，Mobile-Agent-E应运而生。

#### **2.2.1 解决先前智能体的局限性**

先前的移动智能体方法在满足真实世界用户需求方面常常显得力不从心，尤其难以处理需要深度推理和长时程规划的任务，并且普遍缺乏从过往经验中学习和改进的机制 1。Mobile-Agent-E的提出正是为了应对这些显著的局限性。

#### **2.2.2 关键创新**

Mobile-Agent-E引入了多项关键创新以提升其能力：

* **分层多智能体框架 (Hierarchical Multi-Agent Framework):** 该框架明确地将高层规划与低层动作执行分离开来。具体而言，一个“管理者”（Manager）智能体负责制定高层计划并将复杂任务分解为子目标，而四个下属智能体——“感知器”（Perceptor）、“操作员”（Operator）、“行动反思器”（Action Reflector）和“笔记员”（Notetaker）——则分别处理精细化的视觉感知、即时行动执行、错误验证和信息记录 1。这种结构化的任务分配允许更专业化和高效的任务处理。  
* **自我进化模块 (Self-Evolution Module):** 该模块使智能体能够通过持久化的长期记忆从过去的经验中学习。长期记忆包含两种关键知识：“技巧”（Tips）和“捷径”（Shortcuts）1。技巧是从先前任务中总结出的一般性指导和经验教训，而捷径则是为特定子程序定制的可重用、可执行的原子操作序列。

### **2.3 其他迭代版本简介**

在MOBILE-AGENT系列的发展过程中，还出现了如Mobile-Agent-V这样的探索性版本。Mobile-Agent-V据提及通过视频引导的多智能体协作来学习移动设备操作 5。这表明研究者们也在探索利用视频数据作为一种更丰富的操作知识来源，因为视频能够提供比静态截图更连贯和动态的上下文信息，可能有助于智能体学习更复杂的操作序列和环境动态。

### **2.4 演进反映了从反应式到主动式和适应性智能体的成熟过程**

从最初的MOBILE-AGENT到更为先进的Mobile-Agent-E的演进路径，清晰地展示了移动智能体从主要基于当前视觉输入进行反应式操作，向能够主动规划复杂序列、对自身行为进行反思，并通过学习机制不断适应和优化策略的成熟过程。

初始的MOBILE-AGENT模型侧重于利用视觉感知能力，根据当前的屏幕上下文和用户指令进行规划和逐步导航 4。其自我反思机制主要用于即时的错误纠正，例如当一个操作无效或不正确时，尝试替代操作或修改参数 5。

相比之下，Mobile-Agent-E明确地致力于解决先前方法在从*过往经验中学习*以及处理*长时程复杂任务*方面的不足 1。通过引入包含“技巧”和“捷径”的自我进化模块 9，Mobile-Agent-E能够构建一个随时间增长的知识库，用以指导未来的行动，从而提高性能和效率。其分层架构中由“管理者”智能体负责高层规划 9，也标志着系统向更复杂、更主动的关于长远目标的推理能力迈进。

这种演进轨迹与人工智能领域发展的宏观趋势相呼应，即从简单执行命令的工具，发展为能够自主学习、制定策略并持续改进的智能伙伴。这对于解决用户在移动设备上执行复杂任务时常感到的“沮丧”和“耗时”问题至关重要 9。

### **表1：MOBILE-AGENT 与 Mobile-Agent-E 对比概览**

| 特性 | MOBILE-AGENT | Mobile-Agent-E |
| :---- | :---- | :---- |
| **核心架构** | 多模态大语言模型 \+ 视觉工具 | 分层多智能体框架 |
| **主要输入** | 屏幕截图 | 屏幕截图 \+ 长期记忆 (技巧与捷径) |
| **规划机制** | 自我规划 (迭代式) | 管理者进行高层规划，操作员进行低层执行 |
| **学习机制** | 自我反思 (即时错误纠正) | 自我进化模块 (通过技巧与捷径从经验中学习) |
| **关键创新** | 以视觉为中心的操作，无需XML等系统文件；适应性强 | 处理复杂/长时程任务；从过往经验中学习；分层协作 |
| **解决的局限性** | (相对Mobile-Agent-E而言) 对复杂任务和经验学习支持较弱 | 解决了先前智能体在处理真实世界需求、推理密集型/长时程任务以及缺乏经验学习机制的问题 |

## **3\. 架构深度解析：核心技术与机制**

MOBILE-AGENT系列智能体的核心运作依赖于一系列精密协调的技术模块，涵盖视觉感知、规划决策、行动执行以及学习适应等多个层面。

### **3.1 视觉感知与场景理解**

视觉感知是MOBILE-AGENT和Mobile-Agent-E与移动设备GUI交互的基础。

#### **3.1.1 文本定位**

两个系统都利用光学字符识别（OCR）工具来检测和定位屏幕上的文本元素 4。在MOBILE-AGENT中，当屏幕上出现多个相同的文本实例时，系统会裁剪这些文本框周围的区域，并将它们提交给GPT-4V等多模态大语言模型进行判断，以选择正确的交互目标 5。这种方法巧妙地结合了专用视觉模型的高效识别能力和大型语言模型的上下文理解与推理能力。

#### **3.1.2 图标定位**

对于图标的定位，MOBILE-AGENT采用了一种多阶段流程：首先，它请求大语言模型（如GPT-4V）描述目标图标的属性（例如形状、颜色）；然后，利用如Grounding DINO这样的模型来识别屏幕截图中所有的图标；最后，通过CLIP（Contrastive Language–Image Pre-training）模型计算每个检测到的图标与语言模型生成的描述之间的相似度，选择相似度最高的图标作为交互对象 4。这个复杂的过程展示了为解决非平凡的视觉定位任务而设计的精细化策略。

#### **3.1.3 Mobile-Agent-E 中的感知器模块**

Mobile-Agent-E进一步将视觉感知功能模块化，设立了一个专门的“感知器”（Perceptor）模块。这是一个纯粹基于视觉的感知单元，内部集成了OCR模型、图标定位（grounding）模型和图标描述（captioning）模型 9。感知器的输出是一个包含屏幕上所有文本和图标及其精确坐标的精细化列表 9。这种模块化设计有助于提升视觉处理的效率和专业性，使得感知功能可以独立优化和演进。

### **3.2 规划、决策与行动执行**

获取视觉信息后，智能体需要规划行动序列并将其转化为设备操作。

#### **3.2.1 MOBILE-AGENT 中的指令执行与预定义操作**

MOBILE-AGENT能够将其内部规划的动作转化为实际的屏幕操作 5。它拥有一套预定义的原子操作集合，构成了其与移动设备交互的基本行为空间。

##### **表2：MOBILE-AGENT 的预定义操作**

| 操作名称 | 描述 | 参数 |
| :---- | :---- | :---- |
| Open App (App) | 打开指定的应用程序 | App (应用程序名称) |
| Click the text (Text) | 点击屏幕上指定文本所在的区域 | Text (目标文本) |
| Click the icon (Icon, Position) | 点击在指定位置的、符合描述的图标 | Icon (图标描述), Position (位置，如top, bottom, left, right, center) |
| Type (Text) | 在当前输入框中输入给定的文本 | Text (要输入的文本) |
| Page up & down | 向上或向下滚动当前页面 | 无 |
| Back | 返回到前一个页面 | 无 |
| Exit | 返回到设备主屏幕 | 无 |
| Stop | 当指令完成时结束进程 | 无 |

*数据来源: 5*

这个结构化的操作空间为智能体提供了一组具体的交互原语。

#### **3.2.2 MOBILE-AGENT 中的自我规划与自我反思**

MOBILE-AGENT采用迭代的方式执行操作，在每一步都会捕获屏幕截图 5。生成下一个操作的依据包括系统提示、已执行的操作历史以及当前的屏幕截图 5。其自我反思机制用于识别无效操作（如屏幕未发生变化）或不正确操作（如导航到错误页面），并尝试替代操作或修改当前操作的参数以纠正错误 4。智能体会持续反思不正确的操作，并在任务或指令完成后停止执行 4。

#### **3.2.3 Mobile-Agent-E 中的分层任务分解**

Mobile-Agent-E通过其分层多智能体框架实现了更复杂的任务分解和规划：

* **管理者 (Manager):** 这是一个基于LMM的推理智能体，负责高层规划。它将用户的复杂请求分解为一系列子目标 1。在规划过程中，管理者会考虑从长期记忆中提取的“捷径”以优化路径 9。当系统观察到连续的失败动作并触发“错误升级标志”（Error Escalation Flag）时，管理者会介入，审查近期的错误并决定进行更高层次的调整以解决问题 9。  
* **操作员 (Operator):** 这是另一个基于LMM的推理智能体，负责根据管理者制定的高层计划来决定下一个要执行的即时动作（例如，Tap(x, y)) 9。操作员在决策时会参考从长期记忆中获取的“技巧”。其行动空间不仅包括原子操作，还包括可进化的“捷径” 9。在问题升级给管理者之前，操作员会首先尝试自行解决遇到的错误 9。

#### **3.2.4 Mobile-Agent-E 中的行动反思器与笔记员角色**

* **行动反思器 (Action Reflector):** 这是一个基于LMM的智能体，通过比较动作执行前后的屏幕截图来验证上一个动作是否达到了预期的结果。如果动作成功，它会记录当前进展；否则，它会提供额外的错误反馈信息 9。  
* **笔记员 (Notetaker):** 这是一个基于LMM的智能体，负责在任务导航过程中聚合重要的信息，例如产品的价格或餐厅的电话号码 9。

### **3.3 学习与适应：Mobile-Agent-E 中的自我进化模块**

Mobile-Agent-E的核心进步之一在于其引入了自我进化模块，使智能体能够从经验中学习和适应。

#### **3.3.1 长期记忆：技巧与捷径**

该模块维护一个持久化的长期记忆库，存储两种关键类型的知识 1：

* **技巧 (Tips):** 这些是从先前任务中学习到的一般性指导原则和经验教训，涉及如何有效地与环境交互以及从过去的错误中获得的洞见 1。  
* **捷径 (Shortcuts):** 这些是为特定子程序定制的可重用、可执行的原子操作序列。每个捷径都包含一个操作员在使用前必须验证的*前置条件* 9。

#### **3.3.2 经验反思器**

据称，有两个专门的基于LMM的智能体被称为“经验反思器”（Experience Reflectors），它们在每个任务结束时，根据交互历史来更新长期记忆中的技巧和捷径 9。

### **3.4 提示策略**

MOBILE-AGENT的提示方法借鉴了ReAct（Reasoning and Acting）提示框架的思想 5。它要求智能体在输出时包含三个组成部分：观察 (Observation)（对当前屏幕截图和操作历史的描述）、思考 (Thought)（智能体的推理过程和对下一步行动的考量）以及行动 (Action)（选择的操作及其参数）5。这种结构化的思考过程对于基于LMM的智能体执行复杂推理和保持过程透明度至关重要。

### **3.5 架构向专业化和显式知识管理的转变**

Mobile-Agent-E的架构设计，特别是其包含多个分工明确的智能体（如管理者、感知器、操作员等）以及一个显式的自我进化模块（包含技巧、捷径和经验反思器），标志着与早期更偏向单体LMM驱动的方法相比，在架构理念上发生了显著的转变。最初的MOBILE-AGENT虽然也使用了视觉工具，但在很大程度上依赖于一个中心化的大型多模态模型（如GPT-4V）进行规划和反思 4。

Mobile-Agent-E则引入了专业化的智能体角色：感知器专注于视觉处理，管理者负责高层计划，操作员执行低层动作，行动反思器验证结果，笔记员收集信息 9。这是一种明确的劳动分工。更进一步，知识（体现为“技巧”和“捷径”）不再仅仅是LMM通过训练隐式学习到的内容，而是通过专门的机制（自我进化模块、经验反思器）进行显式的组织、存储和检索 9。

这种专业化分工有望通过让每个组件专注于其核心能力来提升处理的鲁棒性和效率。而显式的知识管理（如技巧和捷径）相较于完全依赖单个大模型的涌现学习能力，为经验的积累和复用提供了一种更透明、可能也更可靠的方式。它还为针对智能体特定组件的改进提供了便利。这种设计趋势与复杂人工智能系统的发展方向一致，即倾向于采用模块化和显式知识表示来解决错综复杂的问题。

### **3.6 多层次“反思”的关键作用**

反思机制在MOBILE-AGENT系列中扮演着至关重要的角色，并且其复杂性和深度在Mobile-Agent-E中得到了显著增强。MOBILE-AGENT利用自我反思机制来检测无效或不正确的操作，并尝试替代方案，这是一种反应式的、针对单个操作步骤层面的反思 4。

Mobile-Agent-E则将反思能力提升到了多个层次：

1. **行动后反思：** 其“行动反思器”通过对比操作前后的屏幕状态来验证行动结果是否符合预期 9，提供了一种结构化的、在行动执行之后进行的评估。  
2. **策略性反思：** “管理者”智能体在面对连续失败操作（错误升级）时，会反思*错误模式*，并据此对高层计划进行调整 9。这是一种更宏观的、策略层面的反思。  
3. **元学习式反思：** “经验反思器”在任务结束后，对整个交互历史进行回顾和提炼，从而更新知识库中的“技巧”和“捷径” 9。这是一种元认知层面的反思，旨在实现跨任务的学习和泛化。

这种多层次的反思能力对于确保智能体在复杂环境中的鲁棒性能和实现持续改进至关重要。简单的错误纠正不足以应对复杂任务的需求；智能体需要理解错误发生的*原因*，并将这些学习经验泛化应用到未来的情境中。这种分层的反思机制是更高级智能系统的一个标志，这些系统能够从不同粒度的错误中学习，并相应地调整其行为和知识结构。

## **4\. 能力与操作范围**

MOBILE-AGENT系列智能体的设计目标是赋予移动设备自主执行复杂任务的能力，其操作范围和具体能力随着版本的迭代而不断增强。

### **4.1 支持的任务范围**

* **MOBILE-AGENT:** 该系统旨在根据用户指令执行任务，能够在应用程序内部导航，甚至可以完成涉及多个应用程序联动的操作 8。其配套的Mobile-Eval基准测试包含了在10个常用应用程序上设置的不同难度级别的指令 5，用以评估其基础操作能力。  
* **Mobile-Agent-E:** 此版本特别针对需要深度推理、长时程规划以及跨多个应用程序交互的复杂、多步骤任务而设计 1。其评估基准Mobile-Eval-E包含了诸如“餐厅推荐”、“信息搜索”、“在线购物”、“热点追踪”和“旅行规划”等真实世界场景中的任务 9。

### **4.2 自主导航与交互**

自主导航和交互是MOBILE-AGENT系列的核心能力。两个版本的智能体都能够基于视觉感知和用户指令，通过一系列步骤在移动应用程序中自主导航和执行操作 4。Mobile-Agent-E凭借其分层多智能体架构（特别是管理者和操作员的协同工作），在处理更复杂的目标导向导航任务时，展现出更高的策略性和规划的精细度 9。

### **4.3 错误处理与纠正机制**

有效的错误处理是自主智能体稳定运行的关键。

* **MOBILE-AGENT:** 采用自我反思机制来识别和纠正错误。当检测到无效或不正确的操作时，它会尝试替代操作或修改当前操作的参数 4。智能体会持续反思遇到的无效操作，直到任务完成 4。  
* **Mobile-Agent-E:** 构建了更完善的多级错误处理流程：  
  * 首先，“操作员”会尝试自行解决遇到的执行错误 9。  
  * 如果操作失败，“行动反思器”会提供错误反馈，帮助分析失败原因 9。  
  * 当出现连续的失败动作时，“错误升级标志”会通知“管理者”。管理者随后会审查错误历史，并决定采取更高层次的调整策略来解决问题 9。  
  * 此外，从过去错误中学习到的“技巧”也会被纳入长期记忆，帮助智能体在未来的任务中避免类似错误 9。

### **4.4 信息聚合（Mobile-Agent-E 特有）**

Mobile-Agent-E引入了一个专门的“笔记员”（Notetaker）智能体，其职责是在任务执行过程中聚合和记录遇到的重要信息，例如商品的价格、餐厅的电话号码或地址等 9。这项能力对于那些需要在多个步骤或不同屏幕间收集并保留信息的复杂任务至关重要。

“笔记员”智能体的引入，实际上是针对长时程、信息密集型任务中一个潜在的关键瓶颈所做的设计。许多真实世界的移动任务，如旅行规划（需要记住航班时间、酒店信息）或在线比价购物（需要记录不同平台的价格、规格），都要求用户在任务的不同阶段收集信息，并在后续步骤中回忆和使用这些信息。

虽然大型语言模型本身具有一定的上下文记忆能力，但在非常长的交互序列中，要可靠地保留并精确回忆起分散的、特定的信息片段，而不依赖专门的机制，可能会面临挑战。笔记员智能体充当了一个外部化的、结构化的记忆系统，专门负责识别和存储任务执行过程中的关键信息。这不仅减轻了其他负责规划和执行的智能体（如管理者和操作员）在信息保持方面的认知负担，使它们能更专注于核心的推理和决策任务，同时也使得智能体的信息获取过程更加明确和可靠。因此，笔记员并非仅仅是一个便利性功能，而是确保智能体在长时程、信息密集型任务中能够稳健执行的必要组件，它有效地弥补了单纯依赖LMM隐式记忆可能存在的不足。

## **5\. 性能评估与基准测试概览**

对MOBILE-AGENT系列智能体的能力进行客观评估，离不开标准化的基准测试。随着智能体能力的提升，评估基准也在不断发展。

### **表3：评估基准总结**

| 基准名称 | 提出方/关联智能体 | 核心关注点 | 任务示例/应用数量 | 主要评估指标 |
| :---- | :---- | :---- | :---- | :---- |
| **Mobile-Eval** | MOBILE-AGENT | 基础移动设备操作能力 | 10个常用应用，不同难度指令 5 | 成功率 (Su), 过程得分 (PS) 4 |
| **Mobile-Eval-E** | Mobile-Agent-E | 复杂、长时程、多应用交互任务 | 5类真实世界场景 (如餐厅推荐、在线购物等)，25个任务 9 | 满意度得分 (SS), 满意度得分与步数关系 (SSS), 行动准确率 (AA), 反思准确率 (RA), 终止错误率 (TE) 9 |
| **MobileAgentBench** | 独立研究 (Rawles et al., 2023 3) | 通用移动LLM智能体评估 (易用性、自主性、真实设备支持) | 10个开源应用，100个内置任务 3 | 成功率 (SR), 步骤效率 (SE), 延迟 (Latency), 错误完成率 (FFR), 过度执行率 (OER) 3 |

### **5.1 Mobile-Eval：评估 MOBILE-AGENT 基础能力**

Mobile-Eval是随MOBILE-AGENT一同推出的基准，专门用于评估移动设备操作任务的执行情况 4。它包含了在10个常用移动应用程序上设置的、具有不同难度级别的操作指令 5。该基准主要关注智能体根据用户指令和屏幕视觉信息完成指定任务的能力 4。其核心评估指标包括：任务是否成功完成（Success, Su），以及衡量每一步操作准确性的过程得分（Process Score, PS）4。

### **5.2 Mobile-Eval-E：针对复杂长时程任务的新基准**

随着智能体能力的增强，Mobile-Agent-E的推出伴随着一个新的、更具挑战性的基准——Mobile-Eval-E。该基准特别设计用于评估处理复杂、需要长时程规划和跨多个应用程序交互的移动任务的能力 1。Mobile-Eval-E的出现旨在解决现有基准（如最初的Mobile-Eval）在评估高级智能体时性能已趋于饱和的问题，因为这些早期基准主要集中在相对简单、短时程的任务上 9。

* **任务场景:** Mobile-Eval-E包含25个精心设计的手动构建任务，这些任务分布在5个真实的日常应用场景中：“餐厅推荐”、“信息搜索”、“在线购物”、“热点追踪”和“旅行规划” 9。这些任务被设计为需要深度推理，并且通常涉及比以往基准更多的操作步骤 9。  
* **新颖指标:** 面对许多真实世界任务缺乏明确的二元成功标志或唯一的正确执行轨迹这一挑战，Mobile-Eval-E引入了“满意度得分”（Satisfaction Score, SS）。该指标基于人工编写的评分规则，综合考虑任务关键里程碑的完成情况以及智能体在执行过程中的探索性行为，从而提供更细致的评估 9。此外，还使用了“满意度得分与步数关系曲线”（Satisfaction Score vs Steps, SSS curve）来衡量效率，以及行动准确率（Action Accuracy, AA）、反思准确率（Reflection Accuracy, RA）和终止错误率（Termination Error, TE）等指标 9。

### **5.3 MobileAgentBench：一个对比性基准**

MobileAgentBench是由独立研究提出的，旨在解决现有移动智能体基准测试中存在的诸如可扩展性不足、用户友好性差以及依赖人工验证等问题 3。该基准的目标是实现全自动、可靠、即插即用式的评估，支持在真实设备上运行，并且对智能体的集成代码侵入性极小 3。其初始版本提供了覆盖10个开源应用程序的100个内置基准测试任务 3。

### **5.4 关键性能发现**

* **MOBILE-AGENT on Mobile-Eval:** 根据报告，MOBILE-AGENT在Mobile-Eval基准上取得了“显著的准确率和完成率”，即使是面对具有挑战性的指令，如涉及多应用操作的任务，也能成功完成 8。具体的成功率（Su）和过程得分（PS）数值细节在所提供的材料中并未详细给出，仅提及了这两个度量标准 4。  
* **Mobile-Agent-E on Mobile-Eval-E:** Mobile-Agent-E在Mobile-Eval-E基准上表现出色，相较于先前的SOTA方法（如Mobile-Agent-v2），在使用三种不同的基础模型（GPT-4o, Gemini, Claude）时均实现了22%的绝对性能提升 1。此外，它在任务完成效率（通过SSS曲线衡量）以及行动准确率（AA）、反思准确率（RA）和终止错误率（TE）等细分指标上也展现了强大的性能 9。

### **5.5 对比性能分析（Mobile-Agent vs. AppAgent on MobileAgentBench）**

在MobileAgentBench这个第三方基准上，MOBILE-AGENT与另一个知名的移动智能体AppAgent进行了对比评估，结果揭示了两者之间的一些性能差异。

##### **表4：Mobile-Agent 与 AppAgent 在 MobileAgentBench 上的性能指标**

| 智能体 | 成功率 (SR) | 步骤效率 (SE) | 延迟 (秒) | Tokens (平均每任务) | 错误完成率 (FFR) | 过度执行率 (OER) |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Mobile-Agent | 0.26 | 1.33 | 15.91 | 1236.88 | 0.19 | 0.31 |
| AppAgent | 0.40 | 1.29 | 26.09 | 1505.09 | 0.17 | 0.40 |

*数据来源: 3*

数据显示，AppAgent在该基准上的成功率（0.40）高于MobileAgent（0.26）3。然而，MobileAgent在执行速度上表现更优，其平均延迟（15.91秒）显著低于AppAgent（26.09秒），并且消耗的tokens数量也更少（1236.88 vs 1505.09）3。在错误完成率方面，AppAgent略低（0.17 vs 0.19），但在过度执行率上则高于MobileAgent（0.40 vs 0.31）3。这些对比数据为理解不同智能体在特定基准下的性能特点和潜在权衡（如成功率与效率之间的平衡）提供了有价值的参考。

### **5.6 智能体与基准的协同进化**

MOBILE-AGENT系列的发展与其评估基准的演进之间存在一种紧密的协同关系。最初的MOBILE-AGENT的出现催生了Mobile-Eval基准，用以衡量其核心能力 4。随着智能体技术的发展，例如向Mobile-Agent-E的演进，研究者们发现现有的基准（如Mobile-Eval）在评估这些更高级智能体时，其区分度和挑战性已不足，性能指标趋于“饱和” 9。

这种饱和现象自然地推动了更复杂、更具挑战性的新基准的创建，如Mobile-Eval-E。新基准专门设计了能够测试高级智能体新能力（如长时程规划、多应用协作、深度推理）的任务 9。与此同时，学术界也开始关注基准测试方法论本身存在的一些普遍性问题，例如人工验证的繁琐性、可扩展性不足等，这催生了像MobileAgentBench这样旨在改进基准测试流程和易用性的努力 3。

这种互动表明，智能体的进步驱动了对更优良评估工具的需求，而新的、更完善的基准反过来又通过设定更高的标准和揭示新的挑战来推动智能体技术的发展。例如，Mobile-Eval-E中引入“满意度得分”这样的新颖评价指标 9，也反映出在评估面向复杂、真实世界任务的智能体时，需要比简单的二元成功/失败标签更细致、更多维度的衡量方法。这是一个智能体技术与评估方法学相互促进、共同演进的动态过程。

## **6\. 当前挑战、已识别局限性与未来发展轨迹**

尽管MOBILE-AGENT系列在推动多模态手机智能体发展方面取得了显著成就，但在迈向更广泛、更可靠应用的道路上，仍面临诸多挑战和固有的局限性。

### **6.1 早期 MOBILE-AGENT 方法的局限性（Mobile-Agent-E 已解决）**

Mobile-Agent-E的开发直接回应了早期MOBILE-AGENT方法中存在的一些关键不足。这些不足主要包括：难以有效满足真实世界用户的复杂需求，在处理需要深度推理和长时程规划的任务时表现不佳，以及缺乏从过往经验中学习和改进的机制 1。这些局限性是催生Mobile-Agent-E及其采用分层多智能体架构和自我进化模块的主要驱动因素。

### **6.2 移动 GUI 自动化的固有挑战**

移动GUI自动化本身面临一系列固有难题：

* **UI的动态性与可变性:** 移动应用程序的界面经常更新迭代，并且在不同设备型号、操作系统版本之间也可能存在差异。这种动态性和可变性对依赖视觉感知的智能体系统构成了持续的挑战，要求其感知模块具有高度的鲁棒性和适应性 \[9 (隐性提及), 13\]。  
* **真实世界的复杂性:** 现实世界中的移动应用种类繁多，用户可能执行的任务也千差万别，其复杂程度远超实验室环境 9。智能体需要具备处理这种多样性和复杂性的能力。  
* **特定语言/区域的理解与决策:** 当前为GUI智能体构建的通用多模态大模型在处理特定语言（如中文）的应用程序时，其理解和决策性能可能不佳 6。这揭示了现有通用模型在区域化和本地化支持方面的不足。例如，MobileFlow项目的提出正是为了提升智能体在处理大量中文内容的GUI时的表现 6。

### **6.3 对基础 LMM 和视觉模型的依赖**

MOBILE-AGENT和Mobile-Agent-E等智能体的性能表现，在很大程度上受限于其所依赖的基础大型多模态模型（如GPT-4V, GPT-4o, Gemini, Claude等）和视觉处理模型（如OCR工具, CLIP, Grounding DINO等）的能力 4。这些基础模型中存在的任何偏见、不准确性或能力空白，都可能直接传递并影响到上层智能体的行为和效果。

### **6.4 计算需求**

运行复杂的智能体系统，尤其是像Mobile-Agent-E那样包含多个基于LMM的智能体以及精密视觉处理工具的分层框架，可能会产生巨大的计算开销 \[9 (隐性提及)\]。高昂的计算需求可能成为在资源受限的移动设备上直接部署，或在需要极低延迟的复杂交互场景中实现实时性能的障碍。

### **6.5 未来研究建议**

基于当前面临的挑战和已取得的进展，未来研究可在以下方向重点突破：

* **增强鲁棒性与泛化能力:** 提升智能体在更广泛的应用类型、UI风格和语言环境下的性能。针对特定语言（如6中提及的中文应用问题）的挑战，开发更通用的多语言支持能力。  
* **改进自我进化机制:** 探索更高级的知识获取、表示（超越当前的“技巧”和“捷径”）和迁移学习方法，以加速智能体的适应过程，提升学习效率和效果 9。  
* **应对多语言和多样化UI挑战:** 深入研究类似MobileFlow中采用的混合视觉编码器和对齐训练策略等方法，以期更好地支持多语言GUI环境 6。  
* **降低计算足迹:** 研究模型压缩、知识蒸馏技术，或设计更轻量化、更高效的智能体架构，以利于在端侧设备部署。  
* **提升错误恢复与复杂推理能力:** 开发更先进的策略来处理预料之外的情况，从更广泛的错误类型中恢复，并执行更深层次的因果推理。  
* **伦理考量与用户信任:** 随着智能体自主性的增强，解决隐私保护、数据安全、行为问责等伦理问题，并确保用户对智能体的信任，将变得至关重要（这是一般人工智能伦理的范畴，虽然未在材料中明确提及，但对于此类强大智能体是合乎逻辑的未来关切点）。  
* **人机协作:** 探索智能体如何在其能力边界之外的任务或需要引导时，有效地利用人类的帮助，正如HANNA等系统所尝试的那样 12。

### **6.6 移动智能体开发中的“通用型”与“专用型”之争**

在移动智能体的开发过程中，一个值得关注的现象是“通用型”与“专用型”方法之间的张力。MOBILE-AGENT及其后续版本Mobile-Agent-E的核心设计理念之一是通过以视觉为中心和不依赖特定系统的方式来实现广泛的适用性 5，这体现了对构建“通用型”智能体的追求。其目标是使智能体能够适应多样化的移动环境，而无需针对每个系统或应用进行定制 8。

然而，诸如MobileFlow这类工作的出现，则凸显了在特定场景下“专用型”模型的优势。研究明确指出，当前通用的多模态大模型在处理例如包含大量中文内容的应用程序时，其理解和决策性能尚有不足 6。因此，MobileFlow被提出作为一个“专为GUI智能体设计的、在处理富含中文内容的应用方面表现突出的新型多模态大语言模型” 6。这代表了一种“专用型”的解决思路。

这种现象揭示了一个固有的权衡：通用型智能体可能难以完美处理所有特定UI范式或语言的细微差别；而专用型智能体虽然能在其特定领域内达到更高的性能，但其适用范围相对有限。未来的研究方向可能会探索混合型方法，例如构建一个通用的核心智能体，辅以可插拔的专用模块来处理特定任务或语言；或者研究如何更有效地将通用型智能体针对特定领域或语言进行微调。这反映了人工智能发展中一个经典的挑战，即如何在通用性与专业性之间取得平衡。

### **6.7 动态环境中“真实性标签”的潜在挑战**

Mobile-Eval-E基准测试之所以引入“满意度得分（SS）”这一新颖评价指标，是因为许多真实世界的移动任务“缺乏一个明确的二元成功标志或一个唯一的真实执行轨迹” 9。传统的软件测试或简单任务评估通常依赖于清晰定义预期输出或标准答案。

然而，在移动应用交互中，尤其是对于那些复杂的、探索性的任务（例如“餐厅推荐”、“热点追踪”等），往往存在多种有效的执行路径和可接受的结果状态。在这种情况下，“真实性标签”（ground truth）变得模糊不清或具有多面性。一个简单的成功/失败二元度量标准已不足以全面评估智能体的表现。“满意度得分”通过综合考虑关键里程碑的完成情况和基于人工编写规则的探索性行为评估 9，正是对这一挑战的回应。

这意味着，在动态、开放式的环境中评估高级移动智能体本身就是一个重要的研究课题。当“真实性标签”难以界定或具有主观性时，如何定义“成功”，以及如何创建可扩展的、客观的评估方法，将持续成为该领域的关键问题。这同样也会对智能体的学习方式产生影响，因为在缺乏明确奖励信号的情况下，强化学习等方法的应用将面临更大挑战。

## **7\. 结论：移动智能体的发展图景**

MOBILE-AGENT及其演进版本Mobile-Agent-E代表了多模态手机智能体领域的重要进展，为实现更自主、更智能的移动设备交互奠定了坚实基础。

### **7.1 回顾 MOBILE-AGENT 的贡献**

MOBILE-AGENT的开创性工作主要体现在：

* **引领了以视觉为中心的移动智能体自主操作范式** 4，使智能体能够直接从屏幕内容中理解和行动。  
* **验证了仅通过屏幕截图操作移动设备的可行性**，显著增强了智能体对不同设备和操作系统的适应能力，摆脱了对底层系统细节的依赖 5。  
* **引入了自我规划和自我反思等 foundational concepts**，为智能体在移动环境中进行任务分解和错误修正提供了早期框架 4。

### **7.2 Mobile-Agent-E 进步的意义**

Mobile-Agent-E在其前身的基础上取得了显著突破，其重要性在于：

* **成功应对了早期智能体在处理复杂、长时程任务以及从经验中学习方面的局限性** 1，显著提升了智能体处理真实世界问题的能力。  
* **提出的分层多智能体框架被证明是一种处理复杂任务分解与执行的鲁棒架构** 9，通过明确的角色分工提高了系统的效率和模块化程度。  
* **引入的自我进化模块（包含“技巧”和“捷径”）为智能体的持续改进和效率提升提供了一种有前景的机制** 9，使智能体能够从历史交互中积累知识。  
* **在具有挑战性的新基准测试中取得了领先的性能表现**，推动了该领域的技术前沿 1。

### **7.3 自主移动交互的未来展望**

MOBILE-AGENT系列的发展轨迹预示着，未来的移动智能体将越来越能够理解用户细致入微的需求，并自如地驾驭日益复杂的移动应用生态系统。大型多模态模型、计算机视觉技术以及强化学习等相关领域的持续进步，将是推动这一趋势的核心驱动力。

然而，创建一个不仅功能强大，而且在广泛的移动应用和用户任务中都表现得可靠、值得信赖、高效且真正具有辅助价值的智能体，仍然是一个巨大的挑战。MOBILE-AGENT和Mobile-Agent-E的研究为此项持续的探索事业提供了坚实的基础和清晰的方向。

### **7.4 移动智能体作为通用人工智能挑战的缩影**

深入分析移动智能体在发展过程中所面临的各项挑战——例如复杂的视觉感知、基于不完整信息的推理、多步骤规划、从经验中学习、自然的人机交互以及在动态环境中的评估等——可以发现，这些在很大程度上是通用人工智能（AGI）所面临的宏大挑战在特定领域内的具体体现和缩影。

移动智能体需要在动态变化的视觉环境（即GUI）中进行感知；它们必须理解通常不完全明确的人类指令或目标；它们需要进行推理并规划多步操作以达成这些目标；它们必须从经验中学习以改进性能；它们在与人类共享的环境中运作，引发了人机交互的诸多考量；并且，在开放式场景中评估其性能本身就极具挑战性。

因此，在移动智能体领域取得的进展，不仅能解决移动设备交互的实际问题，更有可能为更广泛的人工智能研究提供有价值的洞见和解决方案。移动设备及其丰富的应用程序和交互模式，为开发和测试更通用的人工智能原理提供了一个受限但高度相关的“微型世界”。反之，通用人工智能领域的任何突破性进展，无疑也将为下一代更强大、更智能的移动智能体的诞生注入新的活力。

#### **引用的著作**

1. \[2501.11733\] Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/abs/2501.11733](https://arxiv.org/abs/2501.11733)  
2. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks | OpenReview, 访问时间为 五月 13, 2025， [https://openreview.net/forum?id=E4jS2ncKYu\&referrer=%5Bthe%20profile%20of%20Haiyang%20Xu%5D(%2Fprofile%3Fid%3D\~Haiyang\_Xu6)](https://openreview.net/forum?id=E4jS2ncKYu&referrer=%5Bthe+profile+of+Haiyang+Xu%5D\(/profile?id%3D~Haiyang_Xu6\))  
3. openreview.net, 访问时间为 五月 13, 2025， [https://openreview.net/pdf?id=BfQNrKJMXq](https://openreview.net/pdf?id=BfQNrKJMXq)  
4. Mobile-Agents: Autonomous Multi-modal Mobile Device Agent With Visual Perception, 访问时间为 五月 13, 2025， [https://www.unite.ai/mobile-agents-autonomous-multi-modal-mobile-device-agent-with-visual-perception/](https://www.unite.ai/mobile-agents-autonomous-multi-modal-mobile-device-agent-with-visual-perception/)  
5. \[Literature Review\] Mobile-Agent: Autonomous Multi-Modal Mobile ..., 访问时间为 五月 13, 2025， [https://www.themoonlight.io/review/mobile-agent-autonomous-multi-modal-mobile-device-agent-with-visual-perception](https://www.themoonlight.io/review/mobile-agent-autonomous-multi-modal-mobile-device-agent-with-visual-perception)  
6. MobileFlow: A Multimodal LLM for Mobile GUI Agent \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2407.04346](https://arxiv.org/html/2407.04346)  
7. Autonomous Multi-Modal Mobile Device Agent with Visual Perception \- Semantic Scholar, 访问时间为 五月 13, 2025， [https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340](https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340)  
8. \[2401.16158\] Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/abs/2401.16158](https://arxiv.org/abs/2401.16158)  
9. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks, 访问时间为 五月 13, 2025， [https://x-plug.github.io/MobileAgent/](https://x-plug.github.io/MobileAgent/)  
10. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks \- ResearchGate, 访问时间为 五月 13, 2025， [https://www.researchgate.net/publication/388231666\_Mobile-Agent-E\_Self-Evolving\_Mobile\_Assistant\_for\_Complex\_Tasks](https://www.researchgate.net/publication/388231666_Mobile-Agent-E_Self-Evolving_Mobile_Assistant_for_Complex_Tasks)  
11. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks \- Papers With Code, 访问时间为 五月 13, 2025， [https://paperswithcode.com/paper/mobile-agent-e-self-evolving-mobile-assistant/review/](https://paperswithcode.com/paper/mobile-agent-e-self-evolving-mobile-assistant/review/)  
12. Daily Papers \- Hugging Face, 访问时间为 五月 13, 2025， [https://huggingface.co/papers?q=Mobile%20agents](https://huggingface.co/papers?q=Mobile+agents)  
13. LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2504.19838v1](https://arxiv.org/html/2504.19838v1)