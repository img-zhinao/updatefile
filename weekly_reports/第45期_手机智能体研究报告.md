# **《智脑时代周刊》第45期**

# **手机智能体的研究进展与前瞻**

##                                                                                                 编制：卢向彤2025.5.13

## **1\. 引言：定义移动智能体及其在人机交互中的重要性**

移动智能体，通常由大型语言模型 (LLM) 或多模态大型语言模型 (MLLM) 驱动，是设计用于通过与图形用户界面 (GUI) 交互来理解、规划和执行移动设备上任务的人工智能系统 1。它们超越了简单的语音助手或脚本自动化，利用先进的人工智能进行感知、推理和行动 1。这些智能体旨在模拟类似人类与移动界面的交互方式，感知视觉信息（如屏幕截图、UI元素）和自然语言指令，以自主导航应用程序并完成复杂的多步骤操作 3。在此背景下，“移动智能体”指的是在移动设备上自主行动的软件实体，通常基于用户指令和屏幕视觉信息进行操作，这要求其具备语义理解和视觉感知能力 6。

移动智能体在人机交互 (HCI) 领域具有举足轻重的意义。它们代表了人机交互方式的范式转变，致力于实现更自然、直观和高效的复杂移动生态系统交互方式 2。这些智能体有望自动化繁琐或复杂的任务，从而提高用户生产力和设备可访问性 7。从命令行界面到图形用户界面的演进普及了计算技术；而基于LLM的GUI智能体则旨在通过允许用户通过简单的对话式命令或高级指令执行复杂的多步骤任务，进一步推动这一进程 2。它们弥合了语言理解与个人设备上真实世界交互之间的鸿沟，在应用测试、个人助理以及复杂操作（例如预订、购物、信息检索）等各种场景中转变用户体验 1。

这种技术的出现不仅仅关乎自动化效率的提升，更深远的影响在于**复杂移动操作的普惠化**。正如GUI相较于命令行界面极大地降低了计算机的使用门槛 2，基于自然语言交互的移动智能体则有望让更广泛的用户群体，包括那些不熟悉复杂技术操作的用户，也能轻松完成以往看来望而却步的多应用、多步骤任务 6。智能体通过抽象化应用程序导航和操作的复杂性，使得用户仅需通过简单的语言指令便可驱动复杂的后台流程。这预示着数字包容性的增强，并可能催生新的应用设计理念，即应用可以假定用户能够通过智能体处理更高层级的任务复杂度。

同时，移动智能体的发展轨迹揭示了**从任务特定型自动化向通用型助手演进的趋势**。早期的自动化工具往往是基于脚本的、功能固化的 2，而当前的智能体则致力于实现动态交互和适应性 1。诸如“自我进化”（如Mobile-Agent-E 9）和处理“长周期、多应用交互” 9 等概念，清晰地指向了智能体能力的拓展。其目标不再仅仅是自动化预定义的任务，而是创造出能够理解并执行新颖、复杂指令，并在多样化的移动环境中学习和适应的通用助手。这将对通用化能力、持续学习能力以及远超当前水平的鲁棒错误处理能力提出极高要求，并可能重塑移动应用的软件开发生命周期，例如，未来应用设计需更多考虑与智能体的交互兼容性。

本报告旨在基于现有认知，深入探讨最新的架构创新（例如分层多智能体系统、验证器驱动模型）、自我进化与学习的细微差别、对领先框架的比较分析，以及对快速发展的基准测试领域的批判性审视。

## **2\. 移动智能体的演进：从脚本操作到自主多模态系统**

移动智能体的发展历程是一部从简单自动化向高度智能化、自主化系统演进的历史。

早期阶段：基于脚本和规则的自动化  
传统的手机自动化依赖于预定义的脚本和模板。这些方法虽然在特定场景下有效，但在面对动态环境和多变的界面时缺乏灵活性 2。这类系统通常用于软件测试或简单的重复性任务，即机器人流程自动化 (RPA) 2。与此同时，像Siri或Google Assistant这样的数字助手虽然能够处理一些简单任务（如设置闹钟），但在执行第三方应用程序内的复杂、多步骤操作方面能力有限 11。  
LLM革命：引入推理和自然语言理解  
大型语言模型 (LLM)，如GPT-3/4的出现，带来了强大的自然语言理解、生成和推理能力，为更智能的智能体铺平了道路 1。基于LLM的智能体能够理解复杂的指令并将其分解为可操作的步骤，这与僵化的脚本相比是一个巨大的飞跃 3。  
多模态飞跃：视觉与语言的融合 (MLLM)  
能够处理和理解文本与视觉信息（如屏幕截图）的多模态大型语言模型 (MLLM)，如GPT-4V和Qwen-VL，已成为现代移动智能体的基石 3。这使得智能体能够“看见”屏幕，通过视觉识别UI元素（图标、文本、按钮），并基于视觉上下文进行操作，而不是仅仅依赖于底层（且通常难以访问）的系统元数据（如XML文件）4。这种以视觉为中心的方法增强了智能体在不同应用程序和操作系统间的适应性 4。  
复杂智能体架构的出现  
近期的进展包括具备自我规划、自我反思和从经验中学习能力的智能体（例如Mobile-Agent, Mobile-Agent-E）5。分层多智能体框架（例如Mobile-Agent-E）和验证器驱动范式（例如V-Droid）代表了前沿技术，旨在在复杂场景中实现更高的鲁棒性、效率和任务完成率 9。  
在这一演进过程中，**“以视觉为中心”的转向是实现泛化的关键驱动因素**。从依赖结构化数据（如XML、系统元数据）到依赖视觉感知，并不仅仅是一种输入方式的替代；它从根本上提升了智能体在高度碎片化且快速变化的移动应用环境中的泛化能力。XML结构在不同应用和版本之间差异巨大，且往往不被外部访问。相比之下，视觉UI虽然多样，但共享通用的交互范式（如按钮、文本框等），MLLM可以通过学习来识别这些范式 4。因此，一个能像人一样“看”的智能体，对于底层代码变化的鲁棒性更强，并有潜力在任何提供视觉界面的应用上操作，无论其内部结构如何。这大大降低了对特定应用集成的需求，为真正通用的移动智能体铺平了道路，但也对视觉感知和定位的鲁棒性与准确性提出了极高要求。

此外，**智能体与基准测试的协同进化**也显著推动了该领域的发展。移动智能体能力的日益复杂化，直接反映在（并且可以说是由）更复杂、更真实的评估基准测试的发展上，同时也受到后者的驱动。这种共生关系加速了领域进展。早期的智能体执行简单的任务，而像Mobile-Agent-E这样的新型智能体则致力于解决“长周期、多应用交互”问题 9。相应地，基准测试也从简单的应用内任务（如Mobile-Eval 6）演变为复杂的、跨应用场景（如Mobile-Eval-E 9）和动态环境（如AndroidWorld 7）。随着智能体能力的增强，研究人员创建了更难的基准来测试其极限。在这些新基准上的成功反过来又标志着新的能力水平，从而鼓励智能体的进一步发展。这个迭代循环对于推动移动智能体能力的边界至关重要，但如果基准不能准确反映多样化的真实世界效用，也存在“追逐基准”的风险。

## **3\. 核心架构范式与使能技术**

现代移动智能体的复杂功能依赖于一套核心架构组件和关键使能技术的协同工作。

**现代移动智能体的基本组件** 12：

* **感知 (Perception)**：此组件负责从移动用户界面提取和解释视觉（屏幕截图）和文本信息 12。它利用光学字符识别 (OCR) 技术进行文本检测和定位，并使用视觉模型（例如图标检测、如Grounding DINO和CLIP这样的定位模型）处理非文本元素 6。重点在于对UI元素及其交互结构的语义理解，而不仅仅是简单的模式匹配 12。例如，Mobile-Agent-E的Perceptor智能体使用OCR、图标定位和图标描述模型 9。  
* **规划 (Planning)**：规划组件负责确定实现用户目标的行动序列，这通常涉及任务分解 12。LLM/MLLM被用于高级规划（将任务分解为子目标）和低级行动选择 5。所采用的技术包括自我规划（基于当前状态和历史记录的迭代决策）和自我反思（错误检测和纠正）5。在Mobile-Agent-E中，Manager负责高级规划，而Operator决定即时行动 9。  
* **行动 (Action)**：行动组件负责在移动设备上执行交互操作（例如点击、滑动、输入文本、打开应用等）12。定义的行动空间通常包括诸如Click(text/icon, position)、Type(text)、Scroll、Navigate Back/Home、Open App等操作 14。除了基于GUI的交互，越来越多地使用系统API来执行更深层次的操作 12。  
* **记忆/学习 (Memory/Learning)**：此组件通过存储和重用来自过去交互、屏幕状态和用户指令的信息来提升性能并从经验中学习 12。短期记忆用于维持会话上下文；长期记忆支持跨任务的持续学习和适应 12。Mobile-Agent-E的自我进化模块及其包含的Tips（从先前任务中总结的通用指南）和Shortcuts（可重用的操作序列）是长期记忆和学习的典型例子 9。AppAgent则通过探索阶段构建知识文档 11。

多模态大型语言模型 (MLLM) 的作用：  
MLLM是核心，为感知、推理和规划提供“大脑” 3。GPT-4V是常用的模型之一 6。它们使智能体能够同时理解指令和视觉屏幕上下文 4。MLLM在少样本或零样本学习方面的能力也得到了利用，通常是通过复杂的提示策略（例如，包含观察、思考、行动的ReAct风格提示）5。  
**关键支持技术**：

* **视觉感知工具**：OCR模型、图标检测/描述/定位模型（例如CLIP, Grounding DINO）对于从屏幕截图中准确识别和定位UI元素至关重要 4。  
* **提示工程 (Prompt Engineering)**：精心设计的提示能够引导MLLM的推理、规划和行动生成 5。这包括系统提示、操作历史和当前屏幕信息。  
* **混合视觉编码器 (Hybrid Visual Encoders)**：MobileFlow引入了新颖的混合视觉编码器（例如，将ViT与像LayoutLMv3这样的UI编码器结合），以支持可变分辨率的图像输入，并更好地支持多语言GUI，特别是对于信息密集的应用程序 13。

在这些组件和技术的基础上，观察到一些更深层次的架构演进趋势。首先，**“感知-规划-行动”循环正变得更加复杂和迭代化**。虽然基本的“感知-规划-行动”周期是AI智能体的基础，但移动智能体正在通过更复杂的子流程（如自我反思、记忆检索/更新和分层规划）来发展这一循环，使其成为一个持续的、自适应的、多层次的过程 5。决策不再是简单的线性流程，而是涉及内部反馈循环（反思）、与记忆（提示/快捷方式）的协商以及专业化子智能体之间的分布式责任。这种智能体“思维过程”内部日益增加的复杂性对于处理真实的移动任务是必要的，但也增加了计算开销以及调试和确保可预测行为的挑战。

其次，**记忆正从简单的历史日志演变为可操作的知识库**。移动智能体中“记忆”的概念正在超越仅仅记录过去的行为。它正在成为一个结构化的、不断发展的知识库（如Mobile-Agent-E的“Tips”和“Shortcuts”），能够主动为未来的规划和执行提供信息，体现了一种持续学习的形式 5。智能体不仅仅是记住它们做了什么，更重要的是学习如何更好或更有效地做事。这是迈向真正适应性和智能的关键一步。这些学习到的知识库的质量和可扩展性将至关重要。如何有效地策划、泛化并防止学习到错误或低效的行为是重要的研究挑战，这也触及了终身学习的原则。

最后，**行动空间的定义是平衡表达能力和复杂性的关键设计选择**。智能体可以执行的操作集合（例如，点击文本、点击图标、输入）是经过精心策划的列表 14。这些操作是原始触摸事件（例如，“点击坐标(x,y)”）的相对高级抽象（例如，“点击文本‘提交’”）。这种设计选择直接影响智能体的能力与LLM需要管理的复杂性之间的平衡。定义一个离散的、某种程度上抽象的行动空间简化了LLM的任务，使其可以从更具语义意义的集合中进行选择，而无需为每个动作输出原始坐标。然而，如果行动空间过于有限，智能体可能无法执行必要的操作。Mobile-Agent-E中引入的“Shortcuts” 9 是朝这个方向迈出的一步，有效地创建了学习到的宏操作。未来的研究可能会探索动态演变的行动空间或能够学习新原子操作的智能体，从而进一步增强灵活性，但也增加了学习挑战。

## **4\. 代表性移动智能体框架深度分析**

近年来，研究领域涌现了多个具有代表性的移动智能体框架，它们在架构设计、核心能力和应用侧重上各有特色。

**Mobile-Agent：开创以视觉为中心的控制** 4

* **核心思想**：一个自主的多模态智能体，完全基于视觉输入（屏幕截图）进行操作，消除了对系统元数据（如XML文件）的依赖，从而增强了适应性。  
* **架构**：利用MLLM（例如GPT-4V）结合视觉感知工具：  
  * 文本定位：使用OCR，并通过GPT-4V消除多个文本实例的歧义。  
  * 图标定位：由LLM生成图标描述，使用Grounding DINO进行图标检测，并利用CLIP进行相似性匹配。  
* **关键能力**：自我规划（基于指令、历史记录和当前屏幕的迭代操作）和自我反思（错误检测与纠正，任务完成情况的重新分析）。采用ReAct风格的提示（观察、思考、行动）。  
* **操作**：定义了8种操作（打开应用、点击文本、点击图标、输入、页面上/下滚动、返回、退出、停止）。  
* **意义**：展示了以视觉为中心的方法在移动智能体自主性方面的可行性和优势。引入了Mobile-Eval基准测试。

**Mobile-Agent-E：分层多智能体架构与自我进化** 9

* **核心思想**：通过引入分层结构和从过去经验中学习的机制，克服先前智能体在处理真实世界、推理密集型、长周期任务方面的局限性。  
* **架构**：分层多智能体框架：  
  * Manager (LMM)：负责高级规划，将任务分解为子目标，并考虑可用的Shortcuts。处理错误升级。  
  * Perceptor (基于视觉)：包含OCR、图标定位和图标描述模型；输出屏幕上的文本/图标及其坐标。  
  * Operator (LMM)：根据Manager的计划和Tips决定下一个即时行动（原子操作或Shortcuts）。  
  * Action Reflector (LMM)：通过比较行动前后的屏幕截图来验证行动结果；记录进展或提供错误反馈。  
  * Notetaker (LMM)：在任务导航过程中聚合重要信息。  
* **自我进化模块**：维护持久的长期记忆：  
  * Tips：从先前任务/错误中学习到的通用指南和经验教训。  
  * Shortcuts：为特定子程序定制的可重用、可执行的原子操作序列（带有前置条件）。  
  * 由Experience Reflectors（基于LLM的智能体）在任务结束后更新。  
* **意义**：代表了向能够学习、适应并处理更复杂、更真实移动任务的智能体迈出的重要一步。在新的Mobile-Eval-E基准测试中，相较于先前的SOTA方法取得了22%的绝对提升。

**V-Droid：以验证器驱动范式提升效率与准确性** 15

* **核心思想**：采用LLM作为**验证器 (verifier)** 来评估候选行动，而不是直接生成行动，旨在改进决策制定并显著降低延迟。  
* **架构/工作流程**：  
  1. 行动空间构建：轻量级行动提取器从UI表示中识别可交互元素和基本行动类型（点击、长按、滚动、输入文本等），并辅以UI无关的默认行动。  
  2. 验证器评分：每个候选行动被格式化为提示，并输入给一个微调过的LLM（验证器，例如Llama-3.1-8B）。验证器对行动的有效性进行评分。  
  3. 行动执行：选择得分最高的行动并执行。  
* **关键创新**：  
  * 采用成对过程偏好训练 (pair-wise progress preference training) 来增强验证器的决策能力。  
  * 提出人-智能体联合标注方法，实现可扩展的训练。  
  * 仅预填充 (prefilling-only) 工作流以加速验证过程。  
* **性能**：在AndroidWorld (59.5% SR)、AndroidLab (38.3% SR) 和MobileAgentBench (49% SR) 等多个公开基准测试中取得了新的SOTA任务成功率，并且延迟极低（0.7秒/步），实现了近乎实时的决策能力。  
* **意义**：挑战了主流的基于生成器的LLM智能体模型，为实现更高效、可能更可靠的智能体提供了一条路径，这对于实际部署至关重要。

**AppAgent：通过探索和文档化经验进行学习** 11

* **核心思想**：将任务分解为探索和部署两个阶段，利用学习到的知识来提高性能。  
* **架构/阶段**：  
  1. 探索阶段：智能体自动与应用程序交互，并将观察结果总结到文档中（知识库）。使用“标记集合” (Set-of-Mark, SoM) 提示进行UI元素定位。  
  2. 部署阶段：采用检索增强生成 (RAG) 技术，利用文档中总结的知识来指导行动生成。  
* **性能**：在MobileAgentBench上，AppAgent (SR: 0.40) 的成功率优于MobileAgent (SR: 0.26)，但延迟和Token使用量更高 11。  
* **意义**：强调了结构化知识获取和检索对于提高智能体性能的价值，特别是对于已知应用程序内的任务。

**MobileFlow：为特定UI/语言环境定制智能体** 13

* **核心思想**：一个专为GUI智能体设计的多模态LLM，特别关注处理包含大量中文内容的应用程序的熟练程度。  
* **架构**：  
  * 基于Qwen-VL-Chat（约210亿参数）。  
  * 混合视觉编码器：将预训练的视觉Transformer (ViT，例如OpenCLIP ViT-B/32) 与能够处理可变分辨率输入的UI编码器（例如LayoutLMv3）相结合。该编码器在GUI页面上进行训练，以提取多样化的UI信息。  
  * 采用可变分辨率图像编码以保持纵横比。  
  * 融合了专家混合 (MoE) 和专门的对齐训练策略。  
* **意义**：解决了MLLM在非英语UI和信息密集型应用上性能不佳的挑战。其专门的视觉编码器设计是提升特定环境下GUI理解能力的关键贡献。

**架构选择的比较性考量**：

* **单体 vs. 分层/多智能体**：Mobile-Agent代表了一种更偏向单体MLLM驱动的方法，而Mobile-Agent-E则展示了在多智能体系统内部署专门角色以处理复杂任务管理和学习的优势。  
* **生成式 vs. 基于验证器**：大多数智能体（Mobile-Agent, Mobile-Agent-E, AppAgent）使用LLM以生成方式产生行动或计划。V-Droid提供了一种独特的基于验证器的替代方案，可能以牺牲部分生成灵活性为代价换取速度和集中的决策准确性。  
* **学习机制**：Mobile-Agent-E明确的自我进化（Tips/Shortcuts）与AppAgent的探索-文档化-RAG方法形成对比。两者都旨在利用过去的经验，但通过不同的机制。MobileFlow则专注于针对特定UI类型的架构适应。

从这些不同的框架设计中，我们可以观察到一些普遍性的趋势和考量。**“不存在万能的”架构**似乎是当前阶段的共识。各种智能体架构（以视觉为中心、分层、验证器驱动、探索聚焦、专用编码器）表明，最优设计可能取决于具体任务或应用场景 4。例如，Mobile-Agent-E适用于复杂的长周期任务，V-Droid追求速度和效率，MobileFlow针对中文应用，而AppAgent则学习特定应用的知识。这意味着未来的研究可能会集中在能够根据手头任务动态采用或混合这些策略的元智能体，或者结合这些方法优点的混合架构。

**智能体中“元认知”能力的重要性日益增加**。诸如Mobile-Agent的“自我反思”、Mobile-Agent-E的“Action Reflector”和“Experience Reflectors”，以及V-Droid的验证步骤等特性，都表明智能体正在发展类似于人类元认知的能力——即思考自身的思维和行动 5。这些机制使智能体能够监控自身表现、从错误中学习并改进策略，这是更高智能的标志。开发强大高效的元认知过程将是创建能够在不可预测的真实环境中有效运作的真正自主和可靠智能体的关键。这也为可解释AI开辟了道路，因为这些反思过程可以提供对智能体决策过程的洞察。

此外，**预计算/探索与实时性能之间的权衡**也是一个核心问题。AppAgent的探索阶段 11 和Mobile-Agent-E的持久记忆 9 都涉及到前期或持续投入以构建知识，这些知识随后有助于部署。这与更具反应性的智能体形成对比。对于在相似环境中重复操作的智能体，投入学习是有益的。而对于不断面临全新场景的智能体，强大的零样本推理和快速适应能力则更为关键。

**表1：代表性移动智能体框架比较概览**

| 智能体名称 | 核心架构方法 | 主要MLLM/技术 | 特色能力/特性 | 主要评估基准 |
| :---- | :---- | :---- | :---- | :---- |
| Mobile-Agent | 以视觉为中心的MLLM | GPT-4V, OCR, CLIP, Grounding DINO | 自我反思、纯视觉操作、无需XML | Mobile-Eval |
| Mobile-Agent-E | 分层多智能体 | LMMs (GPT-4o, Gemini, Claude), OCR, 图标定位/描述 | 自我进化 (Tips/Shortcuts)、处理长周期复杂任务、多智能体协作 | Mobile-Eval-E |
| V-Droid | 验证器驱动的LLM | Llama-3.1-8B (作为验证器) | 近乎实时的验证、低延迟决策、成对过程偏好训练 | AndroidWorld, AndroidLab, MobileAgentBench |
| AppAgent | 探索-RAG (检索增强生成) | LLM, SoM提示 | 通过探索构建知识文档、利用RAG进行部署 | MobileAgentBench |
| MobileFlow | 针对特定UI优化的MLLM，采用混合编码器 | Qwen-VL-Chat, ViT, LayoutLMv3 (UI编码器) | 提升中文UI理解能力、可变分辨率输入、混合视觉编码器 | (特定内部基准，侧重中文应用) |

## **5\. 移动智能体研究中的基准测试与性能评估**

标准化评估对于推动移动智能体的发展和实际应用至关重要 24。基准测试为比较不同智能体架构、衡量进展和确定改进领域提供了手段 11。然而，基准测试本身也面临挑战，包括应用程序状态空间巨大、可行行动序列定义模糊以及仅通过UI操作进行评估效率低下等问题 11。人工评估既繁琐又耗时 11。

**表2：移动智能体评估关键基准**

| 基准名称 | 主要关注点/任务类型 | 主要评估指标 | 特色 |
| :---- | :---- | :---- | :---- |
| Mobile-Eval | 单/多应用视觉操作 | SU, PS, CR, RE | 以视觉为中心，包含多应用任务 |
| Mobile-Eval-E | 长周期、多应用真实世界场景 | SS, SSS, AA, RA, TE | 具挑战性的真实世界任务，引入满意度评分 (SS) |
| MobileAgentBench | 高效评估开源应用上的LLM智能体 | SR, SE, Latency, Tokens, FFR, OER | 低代码侵入性，真实设备执行，简化成功条件定义 |
| AndroidWorld | 动态、参数化的真实世界应用任务 | 奖励信号 (Reward signals) | 动态任务生成，可复现性，覆盖20个真实应用 |
| AndroidLab | LLM/LMM的系统性评估，含微调数据集 | SR, Sub-Goal SR, RRR, ROR | 提供Android Instruct数据集，统一评估和训练环境 |
| AutoEval | 自动化评估信号生成 | 自动生成奖励信号的覆盖率和准确率 | 无需人工定义奖励，结构化子状态表示 |

**关键基准概述**：

* **Mobile-Eval** 4：随Mobile-Agent一同推出，包含10个常用应用程序和不同难度的指令，部分任务需要跨应用操作 16。其重点是基于视觉感知评估移动设备操作能力。  
* **Mobile-Eval-E** 9：随Mobile-Agent-E一同推出，旨在解决现有基准（如AppAgent, Mobile-Agent, Mobile-Agent-v2）在短周期任务上性能已饱和的问题 9。它包含25个精心设计、具挑战性的真实世界任务，这些任务需要长周期、跨应用的交互，涵盖5种场景（餐厅推荐、信息搜索、在线购物、热点趋势、旅行规划）9。  
* **MobileAgentBench** 11：旨在减轻繁琐的人工测试负担，为安卓生态系统中的移动LLM智能体提供了一个高效且用户友好的基准 11。它包含10个开源应用上的100个内置任务，难度各异 11。其优势在于智能体集成代码侵入性低，支持多种安卓操作系统版本，可在真实设备上执行，并简化了第三方开发者指定任务成功条件的流程 11，目标是实现完全自主和可靠的评估 11。  
* **AndroidWorld** 7：一个功能齐全的安卓环境，可为20个真实安卓应用中的116个程序化任务提供奖励信号 7。它能以无限多的方式动态构建参数化的、以自然语言表达的任务，从而提供了一个规模庞大且真实的测试套件 7。通过专用的初始化、成功检查和清理逻辑确保可复现性 7。  
* **AndroidLab** 15：一个系统性的安卓智能体框架，包括操作环境、行动空间和一个包含9个应用（如地图、日历、书籍）中138个任务的可复现基准 27。它同时支持LLM和LMM，并包含了用于模型微调的Android Instruct数据集（9.43万条记录）27。  
* **AutoEval** 24：一个自主的智能体评估框架，无需人工干预即可测试移动智能体。它使用结构化子状态表示来描述UI状态变化，以自动生成奖励信号，并采用一个裁判系统 (Judge System) 来自主评估智能体性能，旨在仅通过任务描述就能提供细粒度的反馈。

性能指标分析：  
评估移动智能体性能的指标多种多样，主要包括：

* **成功率 (Success Rate, SR) / 成功 (Success, SU)**：衡量任务是否完成的二元指标 6。  
* **过程得分 (Process Score, PS) / 步进效率 (Step-wise Efficiency, SE)**：衡量执行过程中每一步的准确性；即正确步骤数/总步骤数 6。  
* **完成率 (Completion Rate, CR)**：智能体完成的人类操作步骤占总人类操作步骤的比例 16。  
* **相对效率 (Relative Efficiency, RE)**：将智能体执行步骤数与人类最优执行步骤数进行比较 16。  
* **满意度评分 (Satisfaction Score, SS) (Mobile-Eval-E)**：用于处理缺乏二元成功标志或真实轨迹的任务，基于人工编写的准则评估里程碑完成情况和探索行为 9。  
* **满意度评分 vs 步骤数 (SSS) 曲线 (Mobile-Eval-E)**：衡量任务完成效率 9。  
* **行动准确率 (Action Accuracy, AA)、反思准确率 (Reflection Accuracy, RA)、终止错误率 (Termination Error, TE) (Mobile-Agent-E)**：行动层面和鲁棒性指标 9。  
* **延迟 (Latency)、Token消耗量、错误完成率 (False Finish Rate, FFR)、过度执行率 (Over Execution Rate, OER) (MobileAgentBench)** 11。

**部分性能报告摘要**：

* Mobile-Agent 在 Mobile-Eval 上的表现：三组指令的SU分别为91%, 82%, 82%；PS约为80%；RE达到人类最优操作的80%左右 16。  
* Mobile-Agent-E 在 Mobile-Eval-E 上的表现：在三种基础模型（GPT-4o, Gemini, Claude）上，相较于先前的SOTA (Mobile-Agent-v2) 取得了22%的绝对提升 9。  
* AppAgent vs. MobileAgent 在 MobileAgentBench 上的表现 11：  
  * AppAgent: SR 0.40, SE 1.29, 延迟 26.09秒, Tokens 1505.09。  
  * MobileAgent: SR 0.26, SE 1.33, 延迟 15.91秒, Tokens 1236.88。  
* V-Droid 在多个基准上的表现 15：  
  * AndroidWorld: 59.5% SR (超越现有方法9.5%)。  
  * AndroidLab: 38.3% SR (超越现有方法2.1%)。  
  * MobileAgentBench: 49% SR (超越现有方法9%)。  
  * 延迟: 0.7 秒/步。  
* AndroidLab：微调开源LLM后，平均SR从4.59%提升至21.50%；LMM从1.93%提升至13.28%。GPT-4o达到了31.16%的SR 27。

**表3：部分代表性智能体在关键基准上的性能总结**

| 智能体名称 | 基准 | 报告的成功率 (SR/SU) 或主要成功指标 | 关键次要指标 (例如 PS/SE, 延迟, SS, SOTA提升) | 来源 |
| :---- | :---- | :---- | :---- | :---- |
| Mobile-Agent | Mobile-Eval | 82%-91% SU | PS \~80%, RE \~80% | 16 |
| Mobile-Agent-E | Mobile-Eval-E | SOTA \+ 22% 绝对提升 (SS) | SSS曲线显示更高效率, 良好的AA, RA | 9 |
| AppAgent | MobileAgentBench | 0.40 SR | SE 1.29, 延迟 26.09s | 11 |
| MobileAgent (对比AppAgent) | MobileAgentBench | 0.26 SR | SE 1.33, 延迟 15.91s | 11 |
| V-Droid | AndroidWorld | 59.5% SR | 延迟 0.7s/步, 超越SOTA 9.5% | 15 |
| V-Droid | AndroidLab | 38.3% SR | 延迟 0.7s/步, 超越SOTA 2.1% | 15 |
| V-Droid | MobileAgentBench | 49% SR | 延迟 0.7s/步, 超越SOTA 9% | 15 |
| AndroidLab 微调LLM | AndroidLab | 平均 21.50% SR | (从4.59%提升) | 27 |
| AndroidLab 微调LMM | AndroidLab | 平均 13.28% SR | (从1.93%提升) | 27 |

基准测试领域的一个显著趋势是\*\*“真实性棘轮”效应：基准不断向更复杂、更动态的真实世界场景推进\*\*。从评估相对简单的单应用任务，到评估复杂、长周期、多应用交互，再到动态环境中的任务（例如 Mobile-Eval \-\> Mobile-Eval-E \-\> AndroidWorld），这种对真实性的追求是推动更强大、更鲁棒智能体发展的必要条件 7。正如9明确指出，现有基准在“短周期、直接任务”上的性能已经“饱和”，因此需要Mobile-Eval-E。7则提到AndroidWorld解决了静态测试集的局限性。随着智能体的改进，基准必须变得更具挑战性，以区分它们并突出新的研究前沿。重点正在从“它能做X吗？”转变为“它能在复杂、不可预测的环境中可靠地做X吗？”。这反过来又需要更复杂的评估基础设施（如AndroidWorld的动态任务生成和AutoEval的自主奖励信号）和能够捕捉简单成功/失败之外细微差别的指标（如Mobile-Eval-E的满意度评分）。这也意味着较早的SOTA结果可能很快就会过时。

**“元基准”或“基准框架”的出现**也标志着该领域的成熟。像AndroidLab和MobileAgentBench这样的系统不仅仅是任务的集合，它们还提供了环境、工具和标准化的智能体集成与评估流程。AutoEval则更进一步，实现了评估本身的自动化 11。这些框架提供了超越任务本身的内容，例如“操作环境”、“行动空间”、“低代码侵入性集成”和“自主裁判系统”。这表明社区认识到需要共享基础设施来减轻建立评估的负担，并确保结果的可比性。这是研究领域走向更标准化实践的成熟标志，可以降低评估新智能体想法的门槛，从而加速研究。然而，就采用哪些框架以及如何确保其持续相关性和发展达成共识将非常重要。

最后，**性能是多维度的——仅靠成功率不足以全面评估**。正在使用的各种指标（SR, PS, SE, RE, SS, SSS, 延迟, Tokens, FFR, OER, AA, RA, TE）表明智能体性能并非单一数字 6。效率、鲁棒性、成本（Token/延迟）以及过程质量正变得与任务完成本身同等重要。Mobile-Eval-E引入SS是因为二元成功标志不足以评估某些真实世界任务 9。MobileAgentBench跟踪FFR和OER 11。V-Droid则强调低延迟 23。以过多的步骤、高延迟或大量错误实现的高成功率，不如以高效和鲁棒的执行实现的略低的成功率。不同的应用场景会对不同维度有不同的侧重（例如，V-Droid的速度，Mobile-Agent-E的复杂推理）。因此，研究人员和开发人员在评估和比较智能体时需要考虑平衡的指标记分卡。这也表明未来的智能体可能会针对特定的性能概况进行优化。

## **6\. 移动智能体发展中的关键挑战与当前局限**

尽管移动智能体取得了显著进展，但其发展仍面临诸多挑战和局限，这些是未来研究需要重点突破的方向。

* **应对真实世界任务的复杂性和长周期规划** 9：当前智能体在处理需要深度推理和在长周期内持续参与的高度复杂、多步骤任务方面仍显不足 20。真实世界的人类需求往往涉及细致入微的指令和智能体难以掌握的隐式上下文 20。长周期规划是其中的一个关键技术挑战 18。  
* **确保鲁棒性、适应性和有效的错误恢复**：  
  * **精确的元素定位**：准确识别UI元素并与之交互，尤其是在动态或混乱的界面中，仍然是一个障碍 18。通用的MLLM可能难以处理GUI特有的视觉语言 18。  
  * **对UI变化的适应性**：如果智能体不是为鲁棒的视觉理解而设计的，那么即使是应用UI的微小变化也可能导致其失效（基于以视觉为中心方法带来的益处推断 4）。  
  * **错误处理和反思**：虽然存在诸如自我反思 5 和行动反射器 9 之类的机制，但要使它们对各种错误类型都真正有效且具有通用性，仍是持续的研究课题。理解反思机制失败的原因也是一个研究点 26。  
* **持续学习、泛化和知识保留机制**：  
  * 许多当前方法缺乏从先前经验中学习和改进的机制，这是一个显著的局限性 20，Mobile-Agent-E直接解决了这个问题。  
  * **有效的知识检索**：高效地组织和检索获取的知识以指导决策是一个挑战 18。  
  * 对未见过的应用、任务和UI变化的泛化能力有限，尤其是对于基于训练的智能体而言 12。  
* **计算需求和设备端效率**：  
  * 运行多个或大型基于MLLM的智能体可能计算量巨大（由多智能体LMM推断 9）。  
  * 实现低延迟的实时交互对于用户体验至关重要。V-Droid的0.7秒/步是这方面的一个显著成就 15。  
* **安全感知的执行控制** 18：确保智能体安全可靠地执行操作，避免意外或有害的后果，至关重要，尤其是在它们获得更多自主权的情况下。  
* **基准测试的可扩展性和可用性** 11：一些现有基准在可扩展性和可用性方面存在问题，需要大量人工来定义任务和进行评估。  
* **特定上下文中的理解和决策**：MLLM可能对特定语言的应用（例如MobileFlow之前的中文应用）的理解和决策能力较差 13。

在这些挑战背后，存在一些更深层次的矛盾和困境。**“泛化-特化困境”** 是其中之一。一方面，研究者致力于构建能够泛化到任何应用/任务的智能体，这需要强大（但可能较慢）的通用MLLM和复杂的推理能力；另一方面，也需要针对特定任务或应用（可能通过微调或像MobileFlow这样的专门架构）实现高效和准确的智能体 12。通用MLLM（如GPT-4V）被用于广泛的适用性 6，而MobileFlow则使用专门的编码器处理中文UI 13。基于训练的智能体在狭窄领域表现出色，但缺乏泛化能力 12。追求极端的泛化可能会牺牲在常见特定场景中的性能，反之亦然。“一个智能体同样出色地适应所有情况”可能是不现实的。这可能导致未来出现分层智能体系统：用于新颖任务的高度通用“探索者”智能体，以及用于常见或关键操作的更专业的“执行者”智能体（可能经过微调或使用学习到的快捷方式）。在这些类型之间转换任务或“即时特化”的能力可能是一个关键的研究方向。

**MLLM的“黑箱”特性对可预测性和信任度构成挑战**。虽然MLLM提供了强大的能力，但其内部决策过程可能不透明。这使得预测智能体在所有情况下的行为、有效调试故障以及建立用户信任变得困难，尤其对于关键任务而言 6。自我反思和行动反射器组件 5 是使过程更透明或可验证的尝试，但核心推理仍在MLLM内部。如果智能体失败或执行了意外操作，理解其原因可能非常困难。这种缺乏可解释性是阻碍智能体在关键场景中部署的障碍。因此，针对基于MLLM智能体的可解释AI (XAI) 研究将变得越来越重要。能够为智能体行为提供理由或突出其决策关键因素的技术，对于调试、改进和建立用户信任至关重要。V-Droid中的验证器模型 15 可能提供一个更受约束且可能更易于解释的决策点。

最后，**自我进化的数据瓶颈**不容忽视。虽然自我进化（如Mobile-Agent-E）前景广阔，但智能体遇到的经验（数据）的质量和多样性将严重影响其学习内容 9。机器学习模型对训练数据非常敏感。如果智能体主要体验狭窄的应用或任务集，或者经常遇到设计不佳的UI，其学习到的策略可能无法很好地泛化，甚至可能在其他情况下适得其反。自我进化的有效性不仅在于学习机制本身，还在于智能体所接触的经验“课程”。这表明需要制定策略以确保进化智能体获得多样化和代表性的经验，可能通过模拟环境、精心策划的任务集，甚至多个智能体之间的协作学习。这也引发了关于如何“忘却”坏习惯或过时快捷方式的问题。

## **7\. 移动智能体的未来方向与新兴趋势**

移动智能体领域正处于快速发展阶段，未来数年有望在多个方向上取得突破性进展。

* **自我进化与自适应学习能力的提升**：在Mobile-Agent-E的Tips/Shortcuts基础上进一步增强自我进化模块，探索更复杂的知识获取、泛化和经验保留方法 9。开发能够实时适应新UI、任务和用户偏好的智能体 12。  
* **向更具泛化能力的智能体迈进**：提高智能体在未见过的应用程序和高度动态界面上有效操作的能力 12。研究通用的、跨平台的智能体，使其能够无缝地在不同操作系统（如安卓、iOS）和设备类型上工作（7提到将桌面Web智能体应用于移动端效果不佳，表明需要针对移动端的通用智能体）。  
* **多智能体协作与AI间交互的潜力**：探索更复杂的多智能体系统，其中专门的智能体协同解决任务（在Mobile-Agent-E的内部多智能体结构基础上扩展）8。AI与AI对话的兴起，例如移动智能体可能与其他AI系统（如预订智能体、支付智能体）交互以满足用户请求 30。  
* **增强推理、规划与决策能力**：改进长周期规划和推理能力，以应对更复杂的任务 18。开发更鲁棒的错误恢复和自我纠正机制 9。  
* **提升效率与设备端部署**：降低延迟和计算成本，使智能体更适用于移动设备上的实时使用（V-Droid是朝此方向迈出的一步 15）。研究模型压缩、量化以及针对设备端MLLM的专用硬件加速。  
* **人-智能体交互与信任**：设计更直观的方式供用户指导、监督智能体并提供反馈。开发可解释AI (XAI) 技术，使智能体决策过程更透明、更值得信赖。  
* **安全性、隐私与伦理考量**：解决安全漏洞：对抗性攻击、越狱、智能体处理敏感信息时的数据隐私问题 12。确保隐私保护的交互策略 12。为自主移动智能体建立伦理准则和安全协议。  
* **自动化评估与基准测试**：持续开发如AutoEval这样复杂且自主的评估框架，以减少人工工作量并提供细粒度的反馈 24。  
* **专业化智能体与工具使用**：智能体将更擅长使用外部工具和API来增强其能力 29。开发针对特定领域（例如MobileFlow针对中文应用 13）或任务类型的专业化智能体。

展望未来，**“智能体经济”——交互式自主系统**的构想逐渐清晰。AI间交互 8 和多智能体协作 29 的趋势表明，未来的单个移动智能体可能成为一个由交互式自主系统组成的更大生态系统的一部分，形成一种“智能体经济”以实现复杂的用户目标。例如，移动智能体可能会协调来自其他专业AI智能体的服务（例如，手机上的旅行规划智能体调用航空公司的预订AI和酒店的预订AI）。这将创造一个新的复杂性和机遇层面，需要用于智能体间通信、协商和信任的协议。移动智能体在此经济体中充当用户的主要界面或协调者。这可能带来极其强大和无缝的任务完成体验，但也引入了系统性风险，例如链中某个智能体失败或行为恶意。智能体间通信协议的标准化将至关重要。

**个性化-隐私悖论将加剧**。随着智能体学习用户偏好和过去行为以提供个性化辅助的能力增强（这是未来发展的一个期望方向），它们将不可避免地处理更多敏感的个人数据，从而加剧深度个性化与用户隐私之间的冲突 12。有效的个性化需要访问和学习用户数据（交互历史、应用使用情况，甚至可能的内容）。智能体对用户了解越多，就越能更好地辅助用户，但如果数据处理不当，或者智能体的学习导致隐私泄露，隐私风险就越大。未来的研究必须高度关注隐私保护机器学习 (PPML) 技术、设备端学习、智能体的联邦学习，以及赋予用户对其智能体学习和共享内容明确控制权的透明数据治理政策。如果处理不当，这一悖论可能成为主要的采用障碍。

此外，一个重要的趋势是**从“被动执行”向“主动辅助”的转变**。凭借增强的学习、推理和情境感知能力，未来的移动智能体可能会从仅仅执行明确的用户命令，发展到主动预测用户需求，并在没有直接指令的情况下提供帮助或自动化日常任务 4。例如，如果一个智能体学习了用户的日常习惯（例如，“每周五，用户X从应用Y订购披萨并向Z发送消息”），它可能会主动发起这些操作或提议这样做。智能体的“智能”可以体现为主动行为，使其感觉更像真正的助手。这引发了关于用户控制、避免过度热心智能体造成干扰以及确保主动行为真正有益并符合用户意图等重大设计挑战。智能体采取主动行为的阈值需要仔细校准，并且必须具备强大的用户否决和反馈机制。

## **8\. 结论与战略展望**

移动智能体领域经历了从基础自动化到由复杂多模态大型语言模型驱动、具备视觉感知、复杂规划乃至学习能力的先进系统的显著演进，Mobile-Agent-E和V-Droid等框架便是其中的杰出代表。当前的SOTA（State-of-the-Art，顶尖水平）系统在特定基准测试中展现了令人印象深刻的性能，但与真正达到人类水平的移动任务管理能力之间仍存在差距。

自我进化、验证器范式、多智能体协作以及对评估真实性的不懈追求，是塑造该领域未来的主导趋势。然而，泛化能力、鲁棒性、长周期规划、运行效率、安全性与信任等关键障碍，依然是整个领域必须克服的研究重点。

移动智能体具有重新定义移动人机交互、释放全新生产力水平和提升数字产品可访问性的巨大潜力。要充分实现这一潜力，持续的跨学科研究（融合人工智能、人机交互、系统工程、伦理学等）至关重要。开发鲁棒、可信且以用户为中心的智能体，将是其广泛应用并发挥变革性影响的关键。在架构进步、MLLM能力提升以及日益严苛的基准测试之间的相互作用驱动下，该领域正蓄势待发，准备迎接快速创新。

从更宏观的视角看，**通过特定领域应用探索通用人工智能 (AGI) 的路径**或许是移动智能体研究的深远意义之一。尽管AGI是一个遥远的目标，但在移动GUI交互这样一个受限但复杂的领域中开发高度能力的智能体，可以作为解决更广泛AGI挑战（如动态环境中的感知、行动、学习和推理）的一个有价值的缩影 9。在移动智能体方面取得成功，可能成为通向更通用人工智能形式的垫脚石或重要进展指标，使其成为超越单纯移动便利性的战略性重要研究领域。

此外，随着智能体理解自然语言指令以执行复杂任务能力的增强，\*\*移动自动化的“开发普惠化”\*\*成为可能。这可以降低非程序员创建复杂移动自动化的门槛，将部分权力从开发者转移给最终用户 1。如果用户可以简单地告诉智能体“帮我预订下周二去伦敦的机票，优先选择英国航空的早班机，并将其添加到我的日历中”，这就绕过了针对个性化复杂工作流进行手动脚本编写或开发者干预的需求。这可能催生用户创建的自动化流程激增，并形成一个类似于IFTTT但功能更强大、更灵活的可共享智能体“技能”或“方案”的新生态系统。这也将改变用户对其定制和控制数字环境能力的认知。

综上所述，本研究报告旨在提供一个更深入、更具分析性的视角，综合最新的研究成果，以期对移动智能体当前的状况及其充满机遇与挑战的未来发展路径，形成一个细致入微的理解。

#### **引用的著作**

1. LLM-Powered GUI Agents in Phone Automation: Surveying ..., 访问时间为 五月 13, 2025， [https://www.preprints.org/manuscript/202501.0413/v1](https://www.preprints.org/manuscript/202501.0413/v1)  
2. Large Language Model-Brained GUI Agents: A Survey \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2411.18279v1](https://arxiv.org/html/2411.18279v1)  
3. LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2504.19838v1](https://arxiv.org/html/2504.19838v1)  
4. arxiv.org, 访问时间为 五月 13, 2025， [https://arxiv.org/abs/2401.16158](https://arxiv.org/abs/2401.16158)  
5. Autonomous Multi-Modal Mobile Device Agent with Visual Perception \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2401.16158v2](https://arxiv.org/html/2401.16158v2)  
6. Mobile-Agents: Autonomous Multi-modal Mobile Device Agent With Visual Perception, 访问时间为 五月 13, 2025， [https://www.unite.ai/mobile-agents-autonomous-multi-modal-mobile-device-agent-with-visual-perception/](https://www.unite.ai/mobile-agents-autonomous-multi-modal-mobile-device-agent-with-visual-perception/)  
7. androidworld:adynamic benchmarking environment for autonomous agents \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/pdf/2405.14573?](https://arxiv.org/pdf/2405.14573)  
8. AI agents at work: The new frontier in business automation | Microsoft Azure Blog, 访问时间为 五月 13, 2025， [https://azure.microsoft.com/en-us/blog/ai-agents-at-work-the-new-frontier-in-business-automation/](https://azure.microsoft.com/en-us/blog/ai-agents-at-work-the-new-frontier-in-business-automation/)  
9. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks, 访问时间为 五月 13, 2025， [https://x-plug.github.io/MobileAgent/](https://x-plug.github.io/MobileAgent/)  
10. \[2501.11733\] Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/abs/2501.11733](https://arxiv.org/abs/2501.11733)  
11. MOBILEAGENTBENCH: AN EFFICIENT AND USER- FRIENDLY BENCHMARK FOR MOBILE LLM AGENTS \- OpenReview, 访问时间为 五月 13, 2025， [https://openreview.net/pdf?id=BfQNrKJMXq](https://openreview.net/pdf?id=BfQNrKJMXq)  
12. Research Trends in Automated Mobile Agents \- Enhans, 访问时间为 五月 13, 2025， [https://www.enhans.ai/newsroom/research-trends-in-automated-mobile-agents](https://www.enhans.ai/newsroom/research-trends-in-automated-mobile-agents)  
13. MobileFlow: A Multimodal LLM for Mobile GUI Agent \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2407.04346](https://arxiv.org/html/2407.04346)  
14. \[Literature Review\] Mobile-Agent: Autonomous Multi-Modal Mobile ..., 访问时间为 五月 13, 2025， [https://www.themoonlight.io/review/mobile-agent-autonomous-multi-modal-mobile-device-agent-with-visual-perception](https://www.themoonlight.io/review/mobile-agent-autonomous-multi-modal-mobile-device-agent-with-visual-perception)  
15. Advancing Mobile GUI Agents: A Verifier-Driven Approach to ... \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2503.15937](https://arxiv.org/html/2503.15937)  
16. Mobile-Agent: Automation of mobile app operations through screenshot analysis, 访问时间为 五月 13, 2025， [https://ai-scholar.tech/en/articles/pattern-recognition/mobile-agent](https://ai-scholar.tech/en/articles/pattern-recognition/mobile-agent)  
17. AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents, 访问时间为 五月 13, 2025， [https://openreview.net/forum?id=il5yUQsrjC](https://openreview.net/forum?id=il5yUQsrjC)  
18. (PDF) A Survey on (M)LLM-Based GUI Agents \- ResearchGate, 访问时间为 五月 13, 2025， [https://www.researchgate.net/publication/390989895\_A\_Survey\_on\_MLLM-Based\_GUI\_Agents](https://www.researchgate.net/publication/390989895_A_Survey_on_MLLM-Based_GUI_Agents)  
19. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks | OpenReview, 访问时间为 五月 13, 2025， [https://openreview.net/forum?id=E4jS2ncKYu\&referrer=%5Bthe%20profile%20of%20Haiyang%20Xu%5D(%2Fprofile%3Fid%3D\~Haiyang\_Xu6)](https://openreview.net/forum?id=E4jS2ncKYu&referrer=%5Bthe+profile+of+Haiyang+Xu%5D\(/profile?id%3D~Haiyang_Xu6\))  
20. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks \- ResearchGate, 访问时间为 五月 13, 2025， [https://www.researchgate.net/publication/388231666\_Mobile-Agent-E\_Self-Evolving\_Mobile\_Assistant\_for\_Complex\_Tasks](https://www.researchgate.net/publication/388231666_Mobile-Agent-E_Self-Evolving_Mobile_Assistant_for_Complex_Tasks)  
21. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks \- Papers With Code, 访问时间为 五月 13, 2025， [https://paperswithcode.com/paper/mobile-agent-e-self-evolving-mobile-assistant/review/](https://paperswithcode.com/paper/mobile-agent-e-self-evolving-mobile-assistant/review/)  
22. Daily Papers \- Hugging Face, 访问时间为 五月 13, 2025， [https://huggingface.co/papers?q=Mobile%20agents](https://huggingface.co/papers?q=Mobile+agents)  
23. Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment \- arXiv, 访问时间为 五月 13, 2025， [http://www.arxiv.org/abs/2503.15937](http://www.arxiv.org/abs/2503.15937)  
24. \[2503.02403\] AutoEval: A Practical Framework for Autonomous Evaluation of Mobile Agents \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/abs/2503.02403](https://arxiv.org/abs/2503.02403)  
25. Autonomous Multi-Modal Mobile Device Agent with Visual Perception \- Semantic Scholar, 访问时间为 五月 13, 2025， [https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340](https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340)  
26. \[PDF\] MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents, 访问时间为 五月 13, 2025， [https://www.semanticscholar.org/paper/a0dd514e1f6a8839c636005114d57efad41889d0](https://www.semanticscholar.org/paper/a0dd514e1f6a8839c636005114d57efad41889d0)  
27. AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents \- Paper Details \- ChatPaper.ai, 访问时间为 五月 13, 2025， [https://www.chatpaper.ai/dashboard/paper/b1127739-eabe-4ca4-acc5-eeff5b4f27d3](https://www.chatpaper.ai/dashboard/paper/b1127739-eabe-4ca4-acc5-eeff5b4f27d3)  
28. AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2410.24024v1](https://arxiv.org/html/2410.24024v1)  
29. Large Language Model Agent: A Survey on Methodology, Applications and Challenges, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2503.21460v1](https://arxiv.org/html/2503.21460v1)  
30. Why the Era of AI Agents Starts Now \- Parloa, 访问时间为 五月 13, 2025， [https://www.parloa.com/resources/blog/why-the-era-of-ai-agents-starts-now/](https://www.parloa.com/resources/blog/why-the-era-of-ai-agents-starts-now/)  
31. Survey on Evaluation of LLM-based Agents \- arXiv, 访问时间为 五月 13, 2025， [https://arxiv.org/html/2503.16416v1](https://arxiv.org/html/2503.16416v1)

本文由深圳智脑时代科技有限公司出品，不构成投资建议！13828710020